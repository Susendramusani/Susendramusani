## Musani Susendranath Reddy - Data Analyst

Welcome to the repository for **Musani Susendranath Reddy**'s resume. This repository contains information about my professional experience, education, technical skills, and data science projects.

## Contact Information

- **Phone:** +1 (551) 465-6084
- **Email:** m.susendra123@gmail.com
- **LinkedIn:** [Musani Susendranath Reddy](https://www.linkedin.com/in/musani-susendranath-reddy-ab6494201)
- **GitHub:** [Susendramusani](https://github.com/Susendramusani)

## Professional Summary

- Data Analyst with around 4 years of experience in leveraging SQL, Python, and SAS for data manipulation, advanced analytics, and predictive modeling to drive actionable insights.
- Proficient in building interactive dashboards using Tableau, Power BI, and Excel (VBA, Pivot Tables, Power Query) for performance monitoring and executive reporting.
- Strong background in ETL development, data transformation, and data warehousing using tools such as Snowflake, Redshift, SQL Server, and Oracle.
- Experienced with big data ecosystems including Hadoop, Spark, AWS, and Azure Databricks to manage and process high-volume datasets efficiently.
- Hands-on in machine learning techniques such as clustering, classification, and regression using Scikit-learn, TensorFlow, and statistical modeling for forecasting and decision support.
- Domain expertise in healthcare and finance, with practical exposure to EHR systems (Epic, Cerner) and credit risk analysis, ensuring compliance with HIPAA and GDPR standards.
- Skilled in conducting A/B testing, data mining, data storytelling, and stakeholder collaboration in Agile and cross-functional environments.
- Adept in using cloud platforms (AWS, Azure) and automation tools like Azure Data Factory, SSIS, and Alteryx to streamline pipelines and improve data quality and availability.

## Core Competencies

- **Programming & Scripting**: Python, SQL, SAS, R, Scala, VBA
- **Data Analysis & Statistical Tools**:  Pandas, NumPy, SciPy, Scikit-learn, TensorFlow, Seaborn, Matplotlib, ggplot2, Excel (Advanced: Pivot Tables, Power Query, VLOOKUP), Regression Analysis, Classification, Clustering, ANOVA, Hypothesis Testing, Confidence Intervals, Probability Distributions, Forecasting, Advanced Analytics
- **Visualization & BI Tools**: Tableau, Power BI, SSRS, Excel (Dashboards, VBA Automation), Data Storytelling
- **Data Engineering & ETL:**: ETL Pipelines, Data Cleaning, Data Wrangling, Data Transformation, Azure Data Factory, Apache Spark, Informatica, SSIS, Alteryx
- **Databases & Data Warehousing**: MySQL, PostgreSQL, SQL Server, Oracle, MongoDB, Snowflake, Amazon Redshift
- **Big Data & Cloud Technologies**: Jira, GitHub, ServiceNow, UAT Testing, Agile & Scrum, SDLC, Waterfall, A/B Testing 
- **Domain Expertise**: Healthcare Analytics (EHR Systems – Epic, Cerner), Financial Analytics (Credit Risk, Loan Performance, Financial Modeling), Regulatory Compliance (HIPAA, GDPR), Supply Chain Analytics

## Education

### University of New Haven (Tagliatela College of Engineering), West Haven, CT  
- **Master of Science in Data Science**  
- **Duration**: Aug 2023 – May 2025  
- **Key Coursework**: Machine Learning, Deep Learning, NLP, Data Visualization, Artificial Intelligence 
- **Achievements**: Dean’s List, 20% Dean Scholarship  

### Hindustan Institute of Technology and Science, Chennai, India  
- **Bachelor of Technology in Electronics and Communication Engineering**  
- **Duration**: July 2017 – Apr 2021  
- **Key Coursework**: Engineering Mathematics, Control Systems, Internet of Things,Signals & Systems,Controls & Systems
- **Achievements**: Dean’s List, 75% Merit Scholarship  

## Professional Experience

### CVS Health, CT | Data Analyst  | May 2024 – Present
- ***Key Contributions***:
- Engineered scalable, end-to-end ETL pipelines using Azure Data Factory, enabling automated ingestion and transformation of high-volume clinical (Epic, Cerner), pharmacy, and third-party health data into Azure-based analytical environments.
- Built parameterized, reusable pipeline templates to support dynamic data orchestration across multiple sources and workflows, significantly reducing manual development effort and improving system maintainability.
- Leveraged Python (Pandas, NumPy) for robust data cleaning, anomaly detection, and transformation of pharmacy claims and clinical encounter data, improving dataset consistency and completeness by 13%.
- Created Power BI dashboards to deliver real-time insights on key health KPIs such as medication adherence, blood pressure control, and HbA1c levels, supporting STAR ratings tracking and care program performance analysis.
- Developed and deployed a logistic regression model to identify members at high risk for uncontrolled diabetes (HbA1c > 8), integrating predictive features from EHRs, refill patterns, and clinical visits to support targeted care interventions.
- Utilized Spark on Azure Databricks to scale predictive analytics workloads across millions of member records, optimizing model performance and accelerating risk stratification processes.
- Reduced end-to-end data pipeline latency by 5% through optimization of data partitioning, query tuning, and scheduling across Azure Data Lake and Databricks layers.
- Collaborated with cross-functional teams in an Agile environment, including pharmacy operations, MinuteClinic, and analytics governance, to refine data definitions, reporting cadence, and business logic alignment across CVS Health units.

#### Deloitte, India | Data Analyst | Jun 2020 – Aug 2023
- ***Key Contributions***:
- Developed interactive Tableau dashboards to visualize credit risk metrics such as default probability, loan delinquency trends, and repayment behavior across customer segments, enabling leadership to drive data-backed lending strategies.
- Performed exploratory data analysis (EDA) using SQL and Python (Pandas, Seaborn) to uncover borrower patterns and financial risk indicators, which led to a 12% increase in model accuracy by incorporating high-impact features like income stability, credit score, and repayment history.
- Designed and deployed scalable ETL pipelines to extract, cleanse, and consolidate data from internal databases, third-party credit bureaus, and CRM systems, ensuring high-quality data flow for real-time risk assessment.
- Applied K-Means clustering to segment over 100,000 borrowers based on loan behavior and financial profiles, identifying four actionable personas that informed targeted credit product strategies and improved risk segmentation.
- Implemented Amazon S3-based data storage architecture with intelligent partitioning, versioning, and retention policies, which enhanced query performance and reduced data retrieval time and cloud storage costs.
- Utilized Apache Spark for distributed ETL of 500,000+ loan records, improving batch processing speed and model training scalability by 5% over traditional ETL tools.
- Led the design and implementation of a centralized data warehouse, integrating siloed customer, transactional, and loan datasets into a single reporting environment, boosting reporting efficiency by 20%.
- Executed A/B testing across multiple alternative scoring models, resulting in a 10% uplift in default prediction performance and guiding enhancements in underwriting policies.
- Automated repetitive financial data preparation and reporting tasks using Excel VBA, reducing manual effort by 20+ hours per week and decreasing reporting errors by 65%, significantly improving operational productivity.

## Data Science Projects

- **Tic-Tac-Toe AI Tournament**
- To boost competitiveness, we created four distinct Tic-Tac-Toe agents that used Minimax, Alpha-Beta,Minimax, Expectimax, and Q-Learning algorithms.
- Algorithmic upgrades resulted in a 30% increase in victory rates over the baseline random agent.
- Conducted a tournament with over 100 matches, keeping track of performance metrics like win rates, drawrates, and agent efficiency.
- Optimized match experience with an average time of less than three minutes, increasing engagement.
- Created an interactive console interface for agent selection and match execution, which improved user experience and accessibility.



- **Image Generation Using Diffusion Probabilistic Models**
- This study focuses on image processing approaches and neural network models for item detection, including preprocessing, autoencoder-based feature extraction, and pre-trained CNNs.
- The InceptionV3 model extracts features, which are then used to calculate cosine similarity and find comparable images.
- The autoencoder develops a compact representation and captures crucial visual information despite significant loss.
- The InceptionV3 model excels at feature extraction and efficient transfer learning for image recognition.
- Future refinements could enhance these tactics for real-world applications.


- **Object Detection of Sunglasses and Cap Using Faster R-CNN**
- The project aims to recognize sunglasses and caps using a fine-tuned Faster R-CNN model, a special dataset,and advanced data preprocessing methods.
- The impact of upgrading to ResNet-50 with an FPN backbone is examined, with a focus on increased feature extraction across several scales and higher performance in complicated scenes with occlusions and overlapping objects.
- Customizing the prediction head with FastRCNNPredictor greatly increased mean Average Precision (mAP) by allowing for greater class distinction and bounding box localiza􀆟on.
- The report compares pre- and post-training performance, analysing training loss, mAP, and the effectiveness of fine-tuning and hyperparameter tweaking.
- Ethical considerations are handled, ensuring that the dataset was anonymized, no personally identifying information was obtained, and that it is only used for educational and scientific purposes.

  

## Certifications

- **Microsoft Excel Dashboards & Data Visualization** 
- **Mastery Data Science Job Simulation**
- **Python 101 for Data Science**
- **Data Analytics Job Simulation**
- **Data Science Bootcamp**

If you'd like to connect or collaborate, feel free to reach out via email or LinkedIn.

